{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2lkMmUL6X6OuD3HvbVFuY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shamshers/GenAI/blob/main/Assignment_1_Implementing_a_Basic_Transformer_Model_from_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMFXBmUM1ivf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Scenario**\n",
        "You are a junior AI engineer at a startup focused on building custom language models. Your team lead has asked you to implement a simplified version of a transformer model to understand its core architecture better. This will help the team debug and optimize larger models in the future.\n",
        "\n",
        "# **Objectives**\n",
        "Implement a basic transformer encoder layer using Python and TensorFlow.\n",
        "Understand the role of self-attention and feed-forward networks in transformers.\n",
        "Validate the implementation by running a forward pass with dummy input.\n",
        "# **Instructions**\n",
        "**Set up your environment:** Install PyTorch or TensorFlow and create a new Python script. Ensure you have the necessary libraries (e.g., torch, numpy). Write a function to generate dummy input tensors of shape (batch_size, sequence_length, embedding_dim) to test your model.\n",
        "\n",
        "**Implement multi-head self-attention:** Create a class for multi-head attention, including query, key, and value linear transformations. Compute scaled dot-product attention and apply softmax to obtain attention weights. Concatenate the outputs of all attention heads.\n",
        "\n",
        "**Build the feed-forward network:** Implement a two-layer MLP with ReLU activation. The hidden layer should have a larger dimension (e.g., 4x the input dimension) as per the original transformer paper.\n",
        "\n",
        "**Combine components into an encoder layer:** Integrate the attention mechanism and feed-forward network with layer normalization and residual connections. Ensure the output shape matches the input shape for stacking multiple layers.\n",
        "\n",
        "**Test the model:** Perform a forward pass with your dummy input and verify the output dimensions. Print intermediate tensors (e.g., attention weights) to debug if necessary.\n",
        "\n",
        "# **Evaluation Criteria**\n",
        "Correct implementation of self-attention and feed-forward layers.\n",
        "Proper handling of residual connections and layer normalization.\n",
        "Successful forward pass with matching input/output dimensions.\n",
        "Clean, modular, and well-commented code.\n",
        "# **Resources**\n",
        "The Illustrated Transformer\n",
        "PyTorch Transformer Documentation\n"
      ],
      "metadata": {
        "id": "S6mCkyxn11tb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Set up your environment: Install PyTorch or TensorFlow and create a new Python script.\n",
        "#Ensure you have the necessary libraries (e.g., torch, numpy).\n",
        "#Write a function to generate dummy input tensors of shape (batch_size, sequence_length, embedding_dim) to test your model.\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.layers import LayerNormalization\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import MultiHeadAttention\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.activations import relu\n",
        "\n",
        "\n",
        "def generate_dummy_input(batch_size: int, sequence_length: int, embedding_dim: int):\n",
        "    \"\"\"\n",
        "    Generates a dummy input tensor with the given shape for transformer model testing in TensorFlow.\n",
        "\n",
        "    Args:\n",
        "        batch_size (int): Number of samples in the batch.\n",
        "        sequence_length (int): Length of each input sequence.\n",
        "        embedding_dim (int): Dimensionality of each embedding vector.\n",
        "\n",
        "    Returns:\n",
        "        tf.Tensor: A random float tensor of shape (batch_size, sequence_length, embedding_dim)\n",
        "    \"\"\"\n",
        "    return tf.random.normal(shape=(batch_size, sequence_length, embedding_dim))\n",
        "\n",
        "# Example usage\n",
        "dummy_input = generate_dummy_input(batch_size=4, sequence_length=10, embedding_dim=512)\n",
        "print(dummy_input.shape)  # Output: (4, 10, 512)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.depth = embed_dim // num_heads  # dimension per head\n",
        "\n",
        "        # Linear layers for Q, K, V\n",
        "        self.wq = Dense(embed_dim)\n",
        "        self.wk = Dense(embed_dim)\n",
        "        self.wv = Dense(embed_dim)\n",
        "\n",
        "        # Output projection layer\n",
        "        self.dense = Dense(embed_dim)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"\n",
        "        Split the last dimension into (num_heads, depth)\n",
        "        Transpose the result to shape (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
        "        \"\"\"Calculate the attention weights.\"\"\"\n",
        "        matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "        # Scale matmul_qk\n",
        "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "        # Add the mask if present\n",
        "        if mask is not None:\n",
        "            scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "        # Softmax to get attention weights\n",
        "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "        # Output\n",
        "        output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "        return output, attention_weights\n",
        "\n",
        "    def call(self, v, k, q, mask=None):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        # Linear projections\n",
        "        q = self.wq(q)  # (batch_size, seq_len_q, embed_dim)\n",
        "        k = self.wk(k)  # (batch_size, seq_len_k, embed_dim)\n",
        "        v = self.wv(v)  # (batch_size, seq_len_v, embed_dim)\n",
        "\n",
        "        # Split heads\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        scaled_attention, attention_weights = self.scaled_dot_product_attention(q, k, v, mask)\n",
        "\n",
        "        # Concatenate heads\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.embed_dim))  # (batch_size, seq_len_q, embed_dim)\n",
        "\n",
        "        # Final linear layer\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, embed_dim)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "\n",
        "class TransformerFeedForward(Layer):\n",
        "    def __init__(self, input_dim, expansion_factor=4, dropout_rate=0.1):\n",
        "        super(TransformerFeedForward, self).__init__()\n",
        "        hidden_dim = input_dim * expansion_factor\n",
        "\n",
        "        self.dense1 = Dense(hidden_dim, activation='relu')  # First layer with ReLU\n",
        "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.dense2 = Dense(input_dim)  # Project back to input_dim\n",
        "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        x = self.dense1(x)\n",
        "        x = self.dropout1(x, training=training)\n",
        "        x = self.dense2(x)\n",
        "        x = self.dropout2(x, training=training)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerEncoderBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_expansion=4, dropout_rate=0.1):\n",
        "        super(TransformerEncoderBlock, self).__init__()\n",
        "        self.mha = MultiHeadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
        "        self.ffn = TransformerFeedForward(input_dim=embed_dim, expansion_factor=ff_expansion, dropout_rate=dropout_rate)\n",
        "\n",
        "        self.norm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.norm2 = LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = Dropout(dropout_rate)\n",
        "        self.dropout2 = Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, training=False, mask=None):\n",
        "        # Multi-head attention + residual connection + norm\n",
        "        attn_output, _ = self.mha(x, x, x, mask)              # Self-attention\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.norm1(x + attn_output)                    # Residual connection\n",
        "\n",
        "        # Feed-forward network + residual connection + norm\n",
        "        ffn_output = self.ffn(out1, training=training)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.norm2(out1 + ffn_output)                  # Residual connection\n",
        "\n",
        "        return out2\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJawU7Cu2Tip",
        "outputId": "e0cd6987-7e2f-48d7-f7fa-bda5b25e4974"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 10, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example dummy input\n",
        "batch_size = 2\n",
        "seq_len = 5\n",
        "embed_dim = 64\n",
        "num_heads = 8\n",
        "\n",
        "mha = MultiHeadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
        "dummy_input = tf.random.normal((batch_size, seq_len, embed_dim))\n",
        "\n",
        "output, attn_weights = mha(dummy_input, dummy_input, dummy_input)\n",
        "print(\"Output shape:\", output.shape)        # (2, 5, 64)\n",
        "print(\"Attention weights shape:\", attn_weights.shape)  # (2, 8, 5, 5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9FfVeiB5cCI",
        "outputId": "ced9c0ca-1649-4215-c8cc-0fe7ce9b62dd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: (2, 5, 64)\n",
            "Attention weights shape: (2, 8, 5, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example input\n",
        "batch_size = 2\n",
        "seq_len = 10\n",
        "embed_dim = 64\n",
        "\n",
        "ffn = TransformerFeedForward(input_dim=embed_dim)\n",
        "dummy_input = tf.random.normal((batch_size, seq_len, embed_dim))\n",
        "\n",
        "output = ffn(dummy_input, training=True)\n",
        "print(\"Output shape:\", output.shape)  # (2, 10, 64)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gM3wwDvx6SpM",
        "outputId": "a3ee73db-d20b-45f9-952d-7c7448455287"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: (2, 10, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dummy input\n",
        "batch_size = 2\n",
        "seq_len = 10\n",
        "embed_dim = 64\n",
        "num_heads = 8\n",
        "\n",
        "encoder_block = TransformerEncoderBlock(embed_dim=embed_dim, num_heads=num_heads)\n",
        "dummy_input = tf.random.normal((batch_size, seq_len, embed_dim))\n",
        "\n",
        "output = encoder_block(dummy_input, training=True)\n",
        "print(\"Output shape:\", output.shape)  # (2, 10, 64)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EE-hC9-J6xwK",
        "outputId": "6b5bc49e-4e0d-4a22-8698-9624e3928170"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: (2, 10, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Setup\n",
        "batch_size = 2\n",
        "seq_len = 10\n",
        "embed_dim = 64\n",
        "num_heads = 8\n",
        "\n",
        "# Generate input\n",
        "dummy_input = generate_dummy_input(batch_size, seq_len, embed_dim)\n",
        "print(\"Input shape:\", dummy_input.shape)\n",
        "\n",
        "# Instantiate layers\n",
        "mha = MultiHeadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
        "ffn = TransformerFeedForward(input_dim=embed_dim)\n",
        "encoder_block = TransformerEncoderBlock(embed_dim=embed_dim, num_heads=num_heads)\n",
        "\n",
        "# Forward pass through attention only (for inspection)\n",
        "attn_output, attn_weights = mha(dummy_input, dummy_input, dummy_input)\n",
        "print(\"Attention output shape:\", attn_output.shape)        # (2, 10, 64)\n",
        "print(\"Attention weights shape:\", attn_weights.shape)      # (2, 8, 10, 10)\n",
        "\n",
        "# Forward pass through feed-forward network\n",
        "ffn_output = ffn(attn_output, training=True)\n",
        "print(\"FFN output shape:\", ffn_output.shape)               # (2, 10, 64)\n",
        "\n",
        "# Full encoder block pass\n",
        "encoder_output = encoder_block(dummy_input, training=True)\n",
        "print(\"Final encoder output shape:\", encoder_output.shape)  # (2, 10, 64)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4hsLMoh7Du_",
        "outputId": "3288860b-6d19-470c-c142-ff2d2d49c16a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: (2, 10, 64)\n",
            "Attention output shape: (2, 10, 64)\n",
            "Attention weights shape: (2, 8, 10, 10)\n",
            "FFN output shape: (2, 10, 64)\n",
            "Final encoder output shape: (2, 10, 64)\n"
          ]
        }
      ]
    }
  ]
}